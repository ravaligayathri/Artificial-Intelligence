{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "error in 1ith iteration is 0.5555555555555556\n",
      "\n",
      "error in 2ith iteration is 0.3333333333333333\n",
      "Distribution summation in 2th iteration is 1.0:\n",
      "alpha value in 2th iteration is -0.6081976621622465: \n",
      "\n",
      "error in 3ith iteration is 0.6554054054054055\n",
      "\n",
      "error in 4ith iteration is 0.7094594594594595\n",
      "\n",
      "error in 5ith iteration is 0.10810810810810814\n",
      "Distribution summation in 5th iteration is 1.0:\n",
      "alpha value in 5th iteration is -0.5291478741970669: \n",
      "\n",
      "error in 6ith iteration is 0.5339403948128608\n",
      "\n",
      "error in 7ith iteration is 0.32743828949441656\n",
      "Distribution summation in 7th iteration is 1.0:\n",
      "alpha value in 7th iteration is -0.6057040694522634: \n",
      "\n",
      "error in 8ith iteration is 0.38263459544287826\n",
      "Distribution summation in 8th iteration is 0.9999999999999999:\n",
      "alpha value in 8th iteration is -0.6302281710947653: \n",
      "\n",
      "error in 9ith iteration is 0.4233066435775366\n",
      "Distribution summation in 9th iteration is 1.0:\n",
      "alpha value in 9th iteration is -0.6501724066552405: \n",
      "\n",
      "error in 10ith iteration is 0.11917778138634219\n",
      "Distribution summation in 10th iteration is 1.0:\n",
      "alpha value in 10th iteration is -0.532395665212566: \n",
      "\n",
      "Final error after ensemble learning : 0.0161229291194864\n",
      "Final voted prediction\n",
      "[-1. -1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class AdaBoost: \n",
    "    \n",
    "    def __init__(self,rounds,X,y,clf_list):\n",
    "        self.rounds=rounds\n",
    "        self.classifier_list=clf_list\n",
    "        self.alpha=[]\n",
    "        self.training_set=X\n",
    "        self.target=y\n",
    "        self.N=len(X)\n",
    "        self.distribution=np.ones(len(X))/len(X)\n",
    "        self.init_distribution=self.distribution\n",
    "        self.hypothesis_list=[]\n",
    "    \n",
    "    def adaboost(self):\n",
    "        for i in range(self.rounds):\n",
    "            \n",
    "            random_index_list=list(np.random.choice(self.N,self.N))\n",
    "            #print('index of the subset chosen for {}ith iteration is :{}'.format((i+1),random_index_list))\n",
    "            error=0.0\n",
    "            if(self.training_set.shape[1]==2):\n",
    "                x_sub1=np.vstack((self.getsubset(self.training_set[:,0],random_index_list)))\n",
    "                x_sub2=np.vstack((self.getsubset(self.training_set[:,1],random_index_list)))\n",
    "                x_sub=np.hstack((x_sub1,x_sub2))\n",
    "                \n",
    "            else:\n",
    "                x_sub=self.getsubset(self.training_set,random_index_list)\n",
    "                x_sub=np.vstack(x_sub)\n",
    "                \n",
    "            y_sub=self.getsubset(self.target,random_index_list)\n",
    "            y_sub=np.vstack(y_sub)\n",
    "           \n",
    "            self.classifier_list[i].fit(x_sub,y_sub)\n",
    "            ypred=self.classifier_list[i].predict(self.training_set)\n",
    "            \n",
    "            for k in range(len(ypred)):\n",
    "                if(ypred[k]!=y_sub[k]):\n",
    "                    error+=self.distribution[k]\n",
    "            \n",
    "                \n",
    "            print(\"\\nerror in {}ith iteration is {}\".format((i+1),error))\n",
    "            \n",
    "            \n",
    "            if(error==0):\n",
    "                self.hypothesis_list.append(self.classifier_list[i])\n",
    "                self.alpha.append(-0.5)\n",
    "                return\n",
    "            if(error>0.5):\n",
    "                self.distribution=self.init_distribution\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            \n",
    "            self.hypothesis_list.append(self.classifier_list[i])       \n",
    "            tmp=0.5*math.log(1-error)/error\n",
    "            self.alpha.append(tmp)\n",
    "            temp_distribution=np.zeros(len(ypred))\n",
    "            norm=self.get_normalization_factor(tmp,self.distribution,self.classifier_list[i],ypred)\n",
    "            \n",
    "           \n",
    "            \n",
    "                    \n",
    "            for l in range(len(ypred)):\n",
    "                self.distribution[l]=self.distribution[l]*math.exp(-tmp*ypred[l]*self.target[l])/norm\n",
    "                \n",
    "            \n",
    "                    \n",
    "                \n",
    "            print(\"Distribution summation in {}th iteration is {}:\".format((i+1),self.distribution.sum()))  \n",
    "            print(\"alpha value in {}th iteration is {}: \".format((i+1),tmp))\n",
    "            \n",
    "    def get_normalization_factor(self,alpha,distribution,hypothesis,ypred):\n",
    "        norm=0.0\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            norm+=distribution[i]*math.exp(-alpha*ypred[i]*self.target[i])\n",
    "        return norm\n",
    "            \n",
    "   \n",
    "    \n",
    "    def getsubset(self,data,index_list):\n",
    "        return np.take(data,index_list)\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        Hx=np.zeros(self.N)\n",
    "         \n",
    "        \n",
    "        for j in range(self.N):\n",
    "            hx=0.0\n",
    "            i=0\n",
    "            final_error=0.0\n",
    "            \n",
    "            for i in range(len(self.hypothesis_list)):\n",
    "                \n",
    "                hx+=((self.alpha[i]*self.hypothesis_list[i].predict((self.training_set[j]).reshape(1, -1))).tolist()[0])\n",
    "            \n",
    "            if(hx!=self.target[j]):\n",
    "                final_error+=self.distribution[j]\n",
    "        \n",
    "                \n",
    "            \n",
    "            Hx[j]=np.sign(hx)\n",
    "        print('\\nFinal error after ensemble learning : {}'.format(final_error))\n",
    "        return Hx\n",
    "            \n",
    "    \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X1=np.asarray([1,2,3,3,4,7,7,9,10])\n",
    "X2=np.asarray([2,4,3,2,1,4,8,11,13])\n",
    "X1=np.vstack(X1)\n",
    "X2=np.vstack(X2)\n",
    "X=np.hstack((X1,X2))\n",
    "Y=[1,1,-1,1,-1,-1,1,-1,1]\n",
    "y=np.asarray(Y).T\n",
    "clf_list=[]\n",
    "for itr in range(10):\n",
    "    h = DecisionTreeClassifier(max_depth=1)\n",
    "    clf_list.append(h)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "ada=AdaBoost(10,X1,y,clf_list)\n",
    "ada.adaboost()\n",
    "x=ada.predict()\n",
    "print('Final voted prediction')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
