{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "error in 1ith iteration is 0.3333333333333333\n",
      "Distribution summation in 1th iteration is 0.9999999999999999:\n",
      "alpha value in 1th iteration is -0.6081976621622465: \n",
      "\n",
      "error in 2ith iteration is 0.25748502994011974\n",
      "Distribution summation in 2th iteration is 0.9999999999999998:\n",
      "alpha value in 2th iteration is -0.5781156420646155: \n",
      "\n",
      "error in 3ith iteration is 0.38958832865665693\n",
      "Distribution summation in 3th iteration is 1.0:\n",
      "alpha value in 3th iteration is -0.6335170256322179: \n",
      "\n",
      "error in 4ith iteration is 0.4562350694035124\n",
      "Distribution summation in 4th iteration is 0.9999999999999999:\n",
      "alpha value in 4th iteration is -0.6676801930028041: \n",
      "\n",
      "error in 5ith iteration is 0.10040226925328163\n",
      "Distribution summation in 5th iteration is 1.0:\n",
      "alpha value in 5th iteration is -0.5269182768513573: \n",
      "\n",
      "error in 6ith iteration is 0.0729504798787046\n",
      "Distribution summation in 6th iteration is 0.9999999999999994:\n",
      "alpha value in 6th iteration is -0.5191761260854031: \n",
      "\n",
      "error in 7ith iteration is 0.7503590105802247\n",
      "Error greater than 0.5 so going back to initial distribution\n",
      "\n",
      "error in 8ith iteration is 0.06888516637750086\n",
      "Distribution summation in 8th iteration is 0.9999999999999999:\n",
      "alpha value in 8th iteration is -0.5180554006007736: \n",
      "\n",
      "error in 9ith iteration is 0.8286685787096966\n",
      "Error greater than 0.5 so going back to initial distribution\n",
      "\n",
      "error in 10ith iteration is 0.026411106215347212\n",
      "Distribution summation in 10th iteration is 0.9999999999999998:\n",
      "alpha value in 10th iteration is -0.5067213869338728: \n",
      "\n",
      "Final error after ensemble learning : 0.08730764821573313\n",
      "Final voted prediction\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "class Stump_ADA:\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self,rounds,X,y,clf_list):\n",
    "        self.rounds=rounds\n",
    "        self.classifier_list=clf_list\n",
    "        self.alpha=[]\n",
    "        self.training_set=X\n",
    "        self.target=y\n",
    "        self.N=len(X)\n",
    "        self.distribution=np.ones(len(X))/len(X)\n",
    "        self.init_distribution=self.distribution\n",
    "        self.hypothesis_list=[]\n",
    "    def adaboost(self):\n",
    "        for i in range(self.rounds):\n",
    "            \n",
    "            random_index_list=list(np.random.choice(self.N,self.N))\n",
    "            #print('index of the subset chosen for {}ith iteration is :{}'.format((i+1),random_index_list))\n",
    "            error=0.0\n",
    "            if(self.training_set.shape[1]==2):\n",
    "                x_sub1=np.vstack((self.getsubset(self.training_set[:,0],random_index_list)))\n",
    "                x_sub2=np.vstack((self.getsubset(self.training_set[:,1],random_index_list)))\n",
    "                x_sub=np.hstack((x_sub1,x_sub2))\n",
    "                \n",
    "            else:\n",
    "                x_sub=self.getsubset(self.training_set,random_index_list)\n",
    "                x_sub=np.vstack(x_sub)\n",
    "                \n",
    "            y_sub=self.getsubset(self.target,random_index_list)\n",
    "            y_sub=np.vstack(y_sub)\n",
    "            \n",
    "            \n",
    "         \n",
    "            \n",
    "           \n",
    "            self.classifier_list[i].fit(x_sub,y_sub)\n",
    "            ypred=self.classifier_list[i].predict(self.training_set)\n",
    "            \n",
    "            for k in range(len(ypred)):\n",
    "                if(ypred[k]!=y_sub[k]):\n",
    "                    error+=self.distribution[k]\n",
    "            \n",
    "                \n",
    "            print(\"\\nerror in {}ith iteration is {}\".format((i+1),error))\n",
    "            \n",
    "            \n",
    "            if(error==0):\n",
    "                self.hypothesis_list.append(self.classifier_list[i])\n",
    "                self.alpha.append(-0.5)\n",
    "                return\n",
    "            if(error>0.5):\n",
    "                self.distribution=self.init_distribution\n",
    "                print('Error greater than 0.5 so going back to initial distribution')\n",
    "                continue\n",
    "            \n",
    "            \n",
    "                   \n",
    "            tmp_alpha=0.5*math.log(1-error)/error\n",
    "            \n",
    "            \n",
    "            norm=self.get_normalization_factor(tmp_alpha,self.distribution,self.classifier_list[i],ypred)\n",
    "            \n",
    "           \n",
    "            for l in range(len(ypred)):\n",
    "                self.distribution[l]=self.distribution[l]*math.exp(-tmp_alpha*ypred[l]*self.target[l])/norm\n",
    "                \n",
    "            self.alpha.append(tmp_alpha)\n",
    "            self.hypothesis_list.append(self.classifier_list[i])\n",
    "                \n",
    "            \n",
    "                    \n",
    "                \n",
    "            print(\"Distribution summation in {}th iteration is {}:\".format((i+1),self.distribution.sum()))  \n",
    "            print(\"alpha value in {}th iteration is {}: \".format((i+1),tmp_alpha))\n",
    "            \n",
    "    def get_normalization_factor(self,alpha,distribution,hypothesis,ypred):\n",
    "        norm=0.0\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            norm+=distribution[i]*math.exp(-alpha*ypred[i]*self.target[i])\n",
    "        return norm\n",
    "            \n",
    "   \n",
    "    \n",
    "    def getsubset(self,data,index_list):\n",
    "        return np.take(data,index_list)\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        Hx=np.zeros(self.N)\n",
    "         \n",
    "        \n",
    "        for j in range(self.N):\n",
    "            hx=0.0\n",
    "            i=0\n",
    "            final_error=0.0\n",
    "            \n",
    "            for i in range(len(self.hypothesis_list)):\n",
    "                \n",
    "                hx+=((self.alpha[i]*self.hypothesis_list[i].predict((self.training_set[j]).reshape(1, -1))).tolist()[0])\n",
    "            \n",
    "            if(hx!=self.target[j]):\n",
    "                final_error+=self.distribution[j]\n",
    "        \n",
    "                \n",
    "            \n",
    "            Hx[j]=np.sign(hx)\n",
    "        print('\\nFinal error after ensemble learning : {}'.format(final_error))\n",
    "        return Hx\n",
    "            \n",
    "    \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X1=np.asarray([1,2,3,3,4,7,7,9,10])\n",
    "X2=np.asarray([2,4,3,2,1,4,8,11,13])\n",
    "X1=np.vstack(X1)\n",
    "X2=np.vstack(X2)\n",
    "X=np.hstack((X1,X2))\n",
    "Y=[1,1,-1,1,-1,-1,1,-1,1]\n",
    "y=np.asarray(Y).T\n",
    "clf_list=[]\n",
    "for itr in range(10):\n",
    "    h = DecisionTreeClassifier(max_depth=1)\n",
    "    clf_list.append(h)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "ada=Stump_ADA(10,X1,y,clf_list)\n",
    "ada.adaboost()\n",
    "x=ada.predict()\n",
    "print('Final voted prediction')\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "error in 1ith iteration is 0.2222222222222222\n",
      "Distribution summation in 1th iteration is 1.0:\n",
      "alpha value in 1th iteration is -0.5654574636320386: \n",
      "\n",
      "error in 2ith iteration is 0.5740352228184395\n",
      "Error greater than 0.5 so going back to initial distribution\n",
      "\n",
      "error in 3ith iteration is 0.5740352228184395\n",
      "Error greater than 0.5 so going back to initial distribution\n",
      "\n",
      "Final error after ensemble learning : 0.1435088057046099\n",
      "Final voted prediction\n",
      "[-1. -1. -1. -1.  1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X1=np.asarray([1,2,3,3,4,7,7,9,10])\n",
    "X2=np.asarray([2,4,3,2,1,4,8,11,13])\n",
    "X1=np.vstack(X1)\n",
    "X2=np.vstack(X2)\n",
    "X=np.hstack((X1,X2))\n",
    "Y=[1,1,-1,1,-1,-1,1,-1,1]\n",
    "y=np.asarray(Y).T\n",
    "clf_list=[]\n",
    "for itr in range(3):\n",
    "    h = DecisionTreeClassifier(max_depth=1)\n",
    "    clf_list.append(h)\n",
    "    \n",
    "ada=Stump_ADA(3,X,y,clf_list)\n",
    "ada.adaboost()\n",
    "x=ada.predict()\n",
    "print('Final voted prediction')\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
